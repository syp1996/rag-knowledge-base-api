# Transformer Basics (Cheat Sheet)
- Positional encoding: sine/cosine functions with different frequencies.
- Q, K, V are linear projections of the input embeddings.
- Multi-head attention = parallel attention heads concatenated + final linear layer.
- LayerNorm is applied before or after sublayers (Pre-LN or Post-LN variants).

## Acronyms
- MHA = Multi-Head Attention
- FFN = Position-wise Feed-Forward Network

DOCID: ML-TRANSFORMER-BASICS
